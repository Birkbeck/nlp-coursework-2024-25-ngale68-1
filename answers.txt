Question 1(d):
The Flesch-Kincaid (FK) score can be unreliable in two key scenarios:

1. Non-standard grammar or literary style:
FK assumes standard sentence structure and punctuation. Texts with irregular grammar, such as
stream-of-consciousness writing (e.g., James Joyce), or those with long, complex sentences 
broken by semicolons or em dashes, can confuse sentence tokenizers and distort average sentence 
length, leading to misleading scores.

2. Vocabulary and conceptual complexity:
FK is based only on surface-level metrics — word length (syllables) and sentence length — and 
does not account for semantic difficulty. A text can have short, simple words and sentences but 
convey complex, abstract, or domain-specific concepts (e.g., legal or philosophical texts), 
which are cognitively demanding despite scoring as “easy.”

Additionally, FK performs poorly on poetry, dialogue-heavy texts, or any content where 
readability depends more on cultural context or background knowledge than linguistic structure.




Question 2(f):
Tokenizer Explanation:

For part 2e, I implemented a custom tokenizer using spaCy. The function works as follows:

- Lemmatization: Each word is reduced to its base form (e.g., "running" → "run"), which helps 
group different forms of the same word together and reduces feature sparsity.
- Lowercasing: All tokens are converted to lowercase to ensure that "Labour" and "labour" are 
treated as the same feature.
- Stopword Removal: Common English stopwords (like "the", "and", "is") are removed, as they do 
not contribute to distinguishing between political parties.
- Alphabetic Filtering: Only alphabetic tokens are kept, removing numbers and punctuation.

The tokenizer is used in a TfidfVectorizer with the following settings:

- ngram_range=(1, 3): This includes unigrams, bigrams, and trigrams, allowing the model to learn 
from both individual words and short phrases.
- max_features=3000: Limits the feature space to the 3000 most informative features, improving 
efficiency and reducing overfitting.
- min_df=3: Ignores terms that appear in fewer than 3 documents, removing rare noise.
- max_df=0.8: Ignores terms that appear in more than 80% of documents, removing overly common, 
non-discriminative terms.


Performance Discussion:

The classifier that performed best with these features was a Linear SVM, which achieved an 
accuracy of 84% and a macro-average F1 score of 0.7823. This outperformed the Random Forest 
classifier, which achieved a macro F1 of 0.67 under the same settings. The SVM showed strong 
performance on the Conservative (F1 = 0.89) and Labour (F1 = 0.75) classes, while the Scottish 
National Party (F1 = 0.71) was more challenging, likely due to its smaller support size (n=136).

To assess the robustness of the setup, I also experimented with relaxed filtering (min_df=2, 
max_df=0.9). This yielded nearly identical results (macro F1 = 0.7826), confirming that the 
original configuration provided an optimal balance between information richness and noise 
reduction.

Overall, the custom tokenizer and vectorization pipeline achieved strong classification 
performance while maintaining efficiency. It captured both surface-level and contextual 
features with high discriminative power, all within the 3000-feature constraint.